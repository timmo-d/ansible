---
- name: Ensure apt cache is up to date
  ansible.builtin.apt:
    update_cache: yes
    cache_valid_time: 3600

# Ensure the keyrings directory exists
- name: Ensure APT keyrings directory exists
  ansible.builtin.file:
    path: /etc/apt/keyrings
    state: directory
    mode: '0755'

# Download Zeek OBS Release key (ASCII)
- name: Fetch Zeek OBS signing key (ASCII)
  ansible.builtin.get_url:
    url: "https://download.opensuse.org/repositories/security:/zeek/xUbuntu_24.04/Release.key"
    dest: /etc/apt/keyrings/zeek-obs.asc
    mode: '0644'
    force: yes

# Convert to a GPG keyring file (dearmored)
- name: Convert Zeek OBS key to keyring (gpg)
  ansible.builtin.command: >
    gpg --dearmor -o /etc/apt/keyrings/zeek-obs.gpg /etc/apt/keyrings/zeek-obs.asc
  args:
    creates: /etc/apt/keyrings/zeek-obs.gpg

# Add the Zeek repository with signed-by
- name: Add Zeek OBS APT source (24.04, signed-by)
  ansible.builtin.apt_repository:
    repo: "deb [arch=amd64 signed-by=/etc/apt/keyrings/zeek-obs.gpg] https://download.opensuse.org/repositories/security:/zeek/xUbuntu_24.04/ /"
    filename: "security:zeek"
    state: present

# Refresh cache *after* key+repo are present
- name: Update APT cache
  ansible.builtin.apt:
    update_cache: yes

- name: Add Zeek OBS GPG key
  ansible.builtin.apt_key:
    url: "{{ zeek_obs_key_url }}"
    state: present

- name: Update apt cache after adding Zeek repo
  ansible.builtin.apt:
    update_cache: yes

# --- Wazuh Agent repository for Ubuntu 24.04 ---
- name: Ensure APT keyrings dir exists
  ansible.builtin.file:
    path: /usr/share/keyrings
    state: directory
    mode: '0755'

- name: Install Wazuh repository GPG key
  ansible.builtin.shell: |
    set -e
    curl -s https://packages.wazuh.com/key/GPG-KEY-WAZUH \
      | gpg --dearmor | tee {{ wazuh_key_path }} > /dev/null
    chmod 644 {{ wazuh_key_path }}
  args:
    creates: "{{ wazuh_key_path }}"

- name: Add Wazuh APT repository (signed-by)
  ansible.builtin.apt_repository:
    repo: "{{ wazuh_repo_line }}"
    filename: wazuh
    state: present

- name: Update apt cache after adding Wazuh repo
  ansible.builtin.apt:
    update_cache: yes

# --- Grafana repo & key (official APT) ---
- name: Ensure APT keyrings directory exists
  ansible.builtin.file:
    path: /etc/apt/keyrings
    state: directory
    mode: '0755'

- name: Download Grafana GPG key (ASCII)
  ansible.builtin.get_url:
    url: https://apt.grafana.com/gpg.key
    dest: /etc/apt/keyrings/grafana.asc
    mode: '0644'

- name: Convert Grafana key to keyring (dearmor)
  ansible.builtin.command: >
    gpg --dearmor -o {{ grafana_keyring_path }} /etc/apt/keyrings/grafana.asc
  args:
    creates: "{{ grafana_keyring_path }}"

- name: Add Grafana APT repository (signed-by)
  ansible.builtin.apt_repository:
    repo: "{{ grafana_repo_line }}"
    filename: grafana
    state: present

- name: Update apt cache after adding Grafana repo
  ansible.builtin.apt:
    update_cache: yes

- name: Install monitoring packages
  ansible.builtin.apt:
    name:
      - "{{ suricata_pkg }}"
      - "{{ zeek_pkg }}"
      - "{{ ossec_pkg }}"
      - "{{ prometheus_pkg }}"
      - "{{ node_exporter_pkg }}"
      - "{{ grafana_pkg }}"
      - "{{ rsyslog_pkg }}"
      - "{{ ufw_pkg }}"
    state: present

- name: Ensure suricata-update is installed
  ansible.builtin.apt:
    name: suricata-update
    state: present
    update_cache: yes

- name: Update Suricata rules
  ansible.builtin.command: suricata-update
  register: suri_update
  changed_when: "'Rules written' in (suri_update.stdout | default(''))"
  failed_when: false

- name: Restart Suricata after rules update
  ansible.builtin.service:
    name: suricata
    state: restarted



- name: Deploy Suricata configuration
  ansible.builtin.template:
    src: suricata.yaml.j2
    dest: "{{ suricata_config_path }}"
    owner: root
    group: root
    mode: '0644'
  notify: restart suricata

- name: Assert monitoring interface exists
  ansible.builtin.assert:
    that:
      - ids_nids_interface in ansible_facts.interfaces
    fail_msg: >-
      Interface {{ ids_nids_interface }} not found on this host.
      Set ids_nids_interface correctly (see `ip -o link`).

- name: Show detected monitoring interface
  ansible.builtin.debug:
    msg: "Using ids_nids_interface='{{ ids_nids_interface }}'"


# Always create the account/dirs first (keep these if you already added them)
- name: Ensure suricata group exists
  ansible.builtin.group:
    name: "{{ suricata_group }}"
    system: yes

- name: Ensure suricata user exists (no shell, no home)
  ansible.builtin.user:
    name: "{{ suricata_user }}"
    group: "{{ suricata_group }}"
    system: yes
    shell: /usr/sbin/nologin
    create_home: no

- name: Ensure Suricata log directory exists
  ansible.builtin.file:
    path: "{{ suricata_log_dir }}"
    state: directory
    owner: "{{ suricata_user }}"
    group: "{{ suricata_group }}"
    mode: '0750'

- name: Ensure Suricata log files exist with correct ownership and permissions
  become: true
  file:
    path: "{{ item.path }}"
    state: touch
    owner: "{{ suricata_user }}"
    group: "{{ suricata_group }}"
    mode: '0644'
  loop:
    - { path: "{{ suricata_log_json }}" }
    - { path: "{{ suricata_log_log }}" }


- name: Ensure Suricata runtime directory exists
  ansible.builtin.file:
    path: /var/run/suricata
    state: directory
    owner: "{{ suricata_user }}"
    group: "{{ suricata_group }}"
    mode: '0750'

# Parse test with full output captured
- name: Validate Suricata configuration (parse test)
  ansible.builtin.command: >
    suricata -T -c {{ suricata_config_path }} -l {{ suricata_log_dir }}
  register: suri_test
  changed_when: false
  failed_when: false

- name: Show Suricata parse test output
  ansible.builtin.debug:
    msg: |
      Suricata -T exit code: {{ suri_test.rc }}
      --- STDERR ---
      {{ suri_test.stderr | default('') }}
      --- STDOUT ---
      {{ suri_test.stdout | default('') }}

# --- Make sure Suricata reference files/paths exist (prevents parse errors) ---
- name: Ensure default rule path exists
  ansible.builtin.file:
    path: /etc/suricata/rules
    state: directory
    owner: "{{ suricata_user | default('suricata') }}"
    group: "{{ suricata_group | default('suricata') }}"
    mode: '0750'

- name: Ensure classification.config exists
  ansible.builtin.copy:
    dest: /etc/suricata/classification.config
    content: |
      # Minimal classification config (extend later)
      config classification: not-suspicious,Not Suspicious Traffic,3
      config classification: unknown,Unknown Traffic,3
    owner: root
    group: root
    mode: '0644'
    force: no  # do not overwrite if package has provided a richer file

- name: Ensure reference.config exists
  ansible.builtin.copy:
    dest: /etc/suricata/reference.config
    content: |
      # Minimal reference config (extend later)
      config reference: url,www.example.com
    owner: root
    group: root
    mode: '0644'
    force: no

# --- Try a parser-only check: should succeed if YAML & paths are good ---
- name: Suricata dump-config (parser validation)
  ansible.builtin.command: >
    suricata --dump-config -c {{ suricata_config_path }}
  register: suri_dump
  changed_when: false
  failed_when: false

- name: Show dump-config (first 80 lines)
  ansible.builtin.debug:
    msg: "{{ (suri_dump.stdout | default(''))[:4000] }}"

# --- Re-run -T but override run-as to root (test-only) to isolate privilege-drop issues ---
- name: Suricata parse test with run-as override (root)
  ansible.builtin.command: >
    suricata -T -c {{ suricata_config_path }} -l {{ suricata_log_dir | default('/var/log/suricata') }}
    --user=root --group=root
  register: suri_test_root
  changed_when: false
  failed_when: false

- name: Show Suricata -T (root override) output
  ansible.builtin.debug:
    msg: |
      Suricata -T (root override) rc={{ suri_test_root.rc }}
      --- STDERR ---
      {{ suri_test_root.stderr | default('') }}
      --- STDOUT ---
      {{ suri_test_root.stdout | default('') }}

# --- If it still fails, print build-info and Suricata log to reveal the cause ---
- name: Suricata build info
  ansible.builtin.command: suricata --build-info
  register: suri_build
  changed_when: false
  failed_when: false

- name: Show build info (first 120 lines)
  ansible.builtin.debug:
    msg: "{{ (suri_build.stdout | default(''))[:6000] }}"

- name: Read suricata.log for errors (last 200 lines)
  ansible.builtin.shell: |
    test -f {{ suricata_log_dir | default('/var/log/suricata') }}/suricata.log && tail -n 200 {{ suricata_log_dir | default('/var/log/suricata') }}/suricata.log || true
  args: { executable: /bin/bash }
  register: suri_log
  changed_when: false
  failed_when: false

- name: Show suricata.log (tail)
  ansible.builtin.debug:
    msg: "{{ suri_log.stdout | default('') }}"


- name: Fail if Suricata parse test failed
  ansible.builtin.fail:
    msg: "Suricata -T failed (rc={{ suri_test.rc }})"
  when: suri_test.rc != 0

- name: Install suricata-update
  ansible.builtin.apt:
    name: suricata-update
    state: present
    update_cache: yes

- name: Update rules with suricata-update
  ansible.builtin.command: suricata-update
  changed_when: "'Rules written' in suri_update.stdout|default('')"
  register: suri_update

# Ensure interface exists
- name: Assert the monitoring interface exists on this host
  ansible.builtin.assert:
    that:
      - ids_nids_interface is defined
      - ids_nids_interface | length > 0
      - ids_nids_interface in ansible_facts.interfaces
    fail_msg: "Interface {{ ids_nids_interface }} not present on this VM."

# Confirm Suricata will really use enp1s0
- name: Check parsed interface in Suricata config
  ansible.builtin.shell: >
    suricata --dump-config -c {{ suricata_config_path }} |
    awk -F': ' '/^af-packet\.0\.interface:/{print $2}'
  args: { executable: /bin/bash }
  register: _suri_nic
  changed_when: false

- name: Fail if parsed interface is not enp1s0
  ansible.builtin.assert:
    that:
      - _suri_nic.stdout == ids_nids_interface
    fail_msg: "Suricata parsed interface '{{ _suri_nic.stdout }}', expected '{{ ids_nids_interface }}'."

# Parse-validation (supported method to verify config)
- name: Validate Suricata configuration (parse test)
  ansible.builtin.command: >
    suricata -T -c {{ suricata_config_path }} -l {{ suricata_log_dir | default('/var/log/suricata') }}
  register: suri_test
  changed_when: false
  failed_when: suri_test.rc != 0


- name: Enable and start Suricata
  ansible.builtin.service:
    name: "{{ suricata_svc }}"
    enabled: yes
    state: started

- name: Create Zeek etc directory
  ansible.builtin.file:
    path: /opt/zeek/etc
    state: directory
    owner: root
    group: root
    mode: '0755'

- name: Deploy Zeek main config (zeekctl.cfg)
  ansible.builtin.template:
    src: zeek.cfg.j2
    dest: "{{ zeek_conf_path }}"
    owner: root
    group: root
    mode: '0644'
  notify: restart zeek

- name: Deploy Zeek networks.cfg
  ansible.builtin.copy:
    dest: "{{ zeek_networks_cfg_path }}"
    content: "{{ zeek_networks }}" # Local networks considered 'home' for Zeek analysis
    owner: root
    group: root
    mode: '0644'
  notify: restart zeek

- name: Enable and start Zeek (via systemd unit if available)
  ansible.builtin.service:
    name: "{{ zeek_svc }}"
    enabled: yes
    state: started
  ignore_errors: yes  # Service name may differ if installed from source

- name: Deploy OSSEC configuration
  ansible.builtin.template:
    src: ossec.conf.j2
    dest: "{{ ossec_conf_path }}"
    owner: root
    group: ossec
    mode: '0640'
  notify: restart ossec

- name: Enable and start Wazuh Agent
  ansible.builtin.service:
    name: "{{ ossec_svc }}"
    enabled: yes
    state: started
  ignore_errors: yes

- name: Deploy Prometheus configuration
  ansible.builtin.template:
    src: prometheus.yml.j2
    dest: "{{ prometheus_config_path }}"
    owner: root
    group: root
    mode: '0644'
  notify: restart prometheus

- name: Install Prometheus alert rules
  ansible.builtin.copy:
    src: prometheus_alert_rules.yml
    dest: /etc/prometheus/alert_rules.yml
    owner: root
    group: root
    mode: '0644'
  notify: restart prometheus

- name: Enable and start Prometheus
  ansible.builtin.service:
    name: "{{ prometheus_svc }}"
    enabled: yes
    state: started

- name: Deploy Grafana ini (hardened)
  ansible.builtin.template:
    src: grafana.ini.j2
    dest: "{{ grafana_ini_path }}"
    owner: root
    group: root
    mode: '0644'
  notify: restart grafana

- name: Ensure Grafana dashboards directory exists
  ansible.builtin.file:
    path: /var/lib/grafana/dashboards
    state: directory
    owner: grafana
    group: grafana
    mode: '0755'
    
- name: Install example Grafana dashboard json
  ansible.builtin.copy:
    src: grafana_dashboard_config.json
    dest: /var/lib/grafana/dashboards/monitoring_dashboard.json
    owner: root
    group: root
    mode: '0644'

- name: Enable and start Grafana
  ansible.builtin.service:
    name: "{{ grafana_svc }}"
    enabled: yes
    state: started

- name: Deploy rsyslog configuration (forward to central server)
  ansible.builtin.template:
    src: rsyslog.conf.j2
    dest: "{{ rsyslog_conf_path }}"
    owner: root
    group: root
    mode: '0644'
  notify: restart rsyslog

- name: Enable and start rsyslog
  ansible.builtin.service:
    name: "{{ rsyslog_svc }}"
    enabled: yes
    state: started

- name: Install logrotate policy for Elasticsearch logs (example)
  ansible.builtin.copy:
    src: elasticsearch_logrotate.conf
    dest: /etc/logrotate.d/elasticsearch
    owner: root
    group: root
    mode: '0644'

- name: Install logrotate policy for Suricata logs
  ansible.builtin.copy:
    src: suricata_logrotate.conf
    dest: /etc/logrotate.d/suricata
    owner: root
    group: root
    mode: '0644'

- name: UFW - ensure enabled with default deny inbound
  community.general.ufw:
    state: enabled
    policy: "{{ ufw_default_input_policy }}"

- name: UFW - allow required TCP service ports
  community.general.ufw:
    rule: allow
    port: "{{ item }}"
    proto: tcp
    from: "{{ mgmt_allow_cidrs | join(',') }}"
  loop: "{{ ufw_allowed_tcp_ports }}"

- name: UFW - allow required UDP service ports
  community.general.ufw:
    rule: allow
    port: "{{ item }}"
    proto: udp
    from: "{{ mgmt_allow_cidrs | join(',') }}"
  loop: "{{ ufw_allowed_udp_ports }}"

- name: Optionally harden SSH daemon (PasswordAuthentication=no)
  ansible.builtin.lineinfile:
    path: /etc/ssh/sshd_config
    regexp: '^PasswordAuthentication'
    line: 'PasswordAuthentication no'
    create: no
  when: ssh_hardening_enable
  notify:
    - restart rsyslog  # placeholder; real deployment should restart ssh
