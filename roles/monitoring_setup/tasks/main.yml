---
- name: Ensure apt cache is up to date
  ansible.builtin.apt:
    update_cache: yes
    cache_valid_time: 3600

# Ensure the keyrings directory exists
- name: Ensure APT keyrings directory exists
  ansible.builtin.file:
    path: /etc/apt/keyrings
    state: directory
    mode: '0755'

# Download Zeek OBS Release key (ASCII)
- name: Fetch Zeek OBS signing key (ASCII)
  ansible.builtin.get_url:
    url: "https://download.opensuse.org/repositories/security:/zeek/xUbuntu_24.04/Release.key"
    dest: /etc/apt/keyrings/zeek-obs.asc
    mode: '0644'
    force: yes

# Convert to a GPG keyring file (dearmored)
- name: Convert Zeek OBS key to keyring (gpg)
  ansible.builtin.command: >
    gpg --dearmor -o /etc/apt/keyrings/zeek-obs.gpg /etc/apt/keyrings/zeek-obs.asc
  args:
    creates: /etc/apt/keyrings/zeek-obs.gpg

# Add the Zeek repository with signed-by
- name: Add Zeek OBS APT source (24.04, signed-by)
  ansible.builtin.apt_repository:
    repo: "deb [arch=amd64 signed-by=/etc/apt/keyrings/zeek-obs.gpg] https://download.opensuse.org/repositories/security:/zeek/xUbuntu_24.04/ /"
    filename: "security:zeek"
    state: present

# Refresh cache *after* key+repo are present
- name: Update APT cache
  ansible.builtin.apt:
    update_cache: yes

- name: Add Zeek OBS GPG key
  ansible.builtin.apt_key:
    url: "{{ zeek_obs_key_url }}"
    state: present

- name: Update apt cache after adding Zeek repo
  ansible.builtin.apt:
    update_cache: yes

# --- Wazuh Agent repository for Ubuntu 24.04 ---
- name: Ensure APT keyrings dir exists
  ansible.builtin.file:
    path: /usr/share/keyrings
    state: directory
    mode: '0755'

- name: Install Wazuh repository GPG key
  ansible.builtin.shell: |
    set -e
    curl -s https://packages.wazuh.com/key/GPG-KEY-WAZUH \
      | gpg --dearmor | tee {{ wazuh_key_path }} > /dev/null
    chmod 644 {{ wazuh_key_path }}
  args:
    creates: "{{ wazuh_key_path }}"

- name: Add Wazuh APT repository (signed-by)
  ansible.builtin.apt_repository:
    repo: "{{ wazuh_repo_line }}"
    filename: wazuh
    state: present

- name: Update apt cache after adding Wazuh repo
  ansible.builtin.apt:
    update_cache: yes

# --- Grafana repo & key (official APT) ---
- name: Ensure APT keyrings directory exists
  ansible.builtin.file:
    path: /etc/apt/keyrings
    state: directory
    mode: '0755'

- name: Download Grafana GPG key (ASCII)
  ansible.builtin.get_url:
    url: https://apt.grafana.com/gpg.key
    dest: /etc/apt/keyrings/grafana.asc
    mode: '0644'

- name: Convert Grafana key to keyring (dearmor)
  ansible.builtin.command: >
    gpg --dearmor -o {{ grafana_keyring_path }} /etc/apt/keyrings/grafana.asc
  args:
    creates: "{{ grafana_keyring_path }}"

- name: Add Grafana APT repository (signed-by)
  ansible.builtin.apt_repository:
    repo: "{{ grafana_repo_line }}"
    filename: grafana
    state: present

- name: Update apt cache after adding Grafana repo
  ansible.builtin.apt:
    update_cache: yes

- name: Install monitoring packages
  ansible.builtin.apt:
    name:
      - "{{ suricata_pkg }}"
      - "{{ zeek_pkg }}"
      - "{{ ossec_pkg }}"
      - "{{ prometheus_pkg }}"
      - "{{ node_exporter_pkg }}"
      - "{{ grafana_pkg }}"
      - "{{ rsyslog_pkg }}"
      - "{{ ufw_pkg }}"
    state: present

# --- Auto-detect the monitoring interface (IDS/NIDS) ---

# Gather just network facts (fast). Safe even if fact gathering is already enabled.
- name: Gather network facts
  ansible.builtin.setup:
    gather_subset:
      - network

# (Optional) Allow an override variable; if set, we honor it and skip detection.
# Usage: set ids_nids_interface_override: "ens160" in group_vars to force a specific NIC.
- name: Use user-provided override if present
  ansible.builtin.set_fact:
    ids_nids_interface: "{{ ids_nids_interface_override }}"
  when:
    - ids_nids_interface is not defined
    - ids_nids_interface_override is defined
    - ids_nids_interface_override | length > 0

# Prefer the default IPv4 route interface discovered by Ansible
- name: Detect interface via default IPv4 route
  ansible.builtin.set_fact:
    ids_nids_interface: "{{ ansible_default_ipv4.interface }}"
  when:
    - ids_nids_interface is not defined
    - ansible_default_ipv4 is defined
    - ansible_default_ipv4.interface is defined
    - ansible_default_ipv4.interface | length > 0

# Fallback 1: Ask the kernel which interface would reach the Internet (no traffic sent)
- name: Fallback detect interface via ip route get
  ansible.builtin.command: bash -lc "ip -o route get 8.8.8.8 | sed -n 's/.* dev \\([^ ]\\+\\) .*/\\1/p'"
  register: _route_dev
  changed_when: false
  failed_when: false
  when: ids_nids_interface is not defined

- name: Set ids_nids_interface from ip route get
  ansible.builtin.set_fact:
    ids_nids_interface: "{{ _route_dev.stdout }}"
  when:
    - ids_nids_interface is not defined
    - _route_dev.stdout is defined
    - _route_dev.stdout | length > 0

# Fallback 2: First active non-loopback link (useful on hosts without a default route)
- name: Fallback detect first non-loopback UP interface
  ansible.builtin.command: bash -lc "ip -o link show up | awk -F': ' '$2!=\"lo\"{print $2; exit}'"
  register: _uplink
  changed_when: false
  failed_when: false
  when: ids_nids_interface is not defined

- name: Set ids_nids_interface from UP link
  ansible.builtin.set_fact:
    ids_nids_interface: "{{ _uplink.stdout }}"
  when:
    - ids_nids_interface is not defined
    - _uplink.stdout is defined
    - _uplink.stdout | length > 0

# Validate that we discovered something sane
- name: Assert monitoring interface was detected
  ansible.builtin.assert:
    that:
      - ids_nids_interface is defined
      - ids_nids_interface | length > 0
    fail_msg: >-
      Could not auto-detect a monitoring interface.
      Set `ids_nids_interface_override` (or `ids_nids_interface`) in group_vars/all/main.yml.

# Optional: print what we detected (useful in first run logs)
- name: Show detected monitoring interface
  ansible.builtin.debug:
    msg: "Using ids_nids_interface='{{ ids_nids_interface }}'"

- name: Deploy Suricata configuration
  ansible.builtin.template:
    src: suricata.yaml.j2
    dest: "{{ suricata_config_path }}"
    owner: root
    group: root
    mode: '0644'
  notify: restart suricata

- name: Assert monitoring interface exists
  ansible.builtin.assert:
    that:
      - ids_nids_interface in ansible_facts.interfaces
    fail_msg: >-
      Interface {{ ids_nids_interface }} not found on this host.
      Set ids_nids_interface correctly (see `ip -o link`).
# --- Ensure Suricata service account and paths exist ---

# Always create the account/dirs first (keep these if you already added them)
- name: Ensure suricata group exists
  ansible.builtin.group:
    name: "{{ suricata_group }}"
    system: yes

- name: Ensure suricata user exists (no shell, no home)
  ansible.builtin.user:
    name: "{{ suricata_user }}"
    group: "{{ suricata_group }}"
    system: yes
    shell: /usr/sbin/nologin
    create_home: no

- name: Ensure Suricata log directory exists
  ansible.builtin.file:
    path: "{{ suricata_log_dir }}"
    state: directory
    owner: "{{ suricata_user }}"
    group: "{{ suricata_group }}"
    mode: '0750'

- name: Ensure Suricata runtime directory exists
  ansible.builtin.file:
    path: /var/run/suricata
    state: directory
    owner: "{{ suricata_user }}"
    group: "{{ suricata_group }}"
    mode: '0750'

# Parse test with full output captured
- name: Validate Suricata configuration (parse test)
  ansible.builtin.command: >
    suricata -T -c {{ suricata_config_path }} -l {{ suricata_log_dir }}
  register: suri_test
  changed_when: false
  failed_when: false

- name: Show Suricata parse test output
  ansible.builtin.debug:
    msg: |
      Suricata -T exit code: {{ suri_test.rc }}
      --- STDERR ---
      {{ suri_test.stderr | default('') }}
      --- STDOUT ---
      {{ suri_test.stdout | default('') }}

- name: Fail if Suricata parse test failed
  ansible.builtin.fail:
    msg: "Suricata -T failed (rc={{ suri_test.rc }})"
  when: suri_test.rc != 0

- name: Enable and start Suricata
  ansible.builtin.service:
    name: "{{ suricata_svc }}"
    enabled: yes
    state: started

- name: Create Zeek etc directory
  ansible.builtin.file:
    path: /opt/zeek/etc
    state: directory
    owner: root
    group: root
    mode: '0755'

- name: Deploy Zeek main config (zeekctl.cfg)
  ansible.builtin.template:
    src: zeek.cfg.j2
    dest: "{{ zeek_conf_path }}"
    owner: root
    group: root
    mode: '0644'
  notify: restart zeek

- name: Deploy Zeek networks.cfg
  ansible.builtin.copy:
    dest: "{{ zeek_networks_cfg_path }}"
    content: "{{ zeek_networks }}" # Local networks considered 'home' for Zeek analysis
    owner: root
    group: root
    mode: '0644'
  notify: restart zeek

- name: Enable and start Zeek (via systemd unit if available)
  ansible.builtin.service:
    name: "{{ zeek_svc }}"
    enabled: yes
    state: started
  ignore_errors: yes  # Service name may differ if installed from source

- name: Deploy OSSEC configuration
  ansible.builtin.template:
    src: ossec.conf.j2
    dest: "{{ ossec_conf_path }}"
    owner: root
    group: ossec
    mode: '0640'
  notify: restart ossec

- name: Enable and start Wazuh Agent
  ansible.builtin.service:
    name: "{{ ossec_svc }}"
    enabled: yes
    state: started
  ignore_errors: yes

- name: Deploy Prometheus configuration
  ansible.builtin.template:
    src: prometheus.yml.j2
    dest: "{{ prometheus_config_path }}"
    owner: root
    group: root
    mode: '0644'
  notify: restart prometheus

- name: Install Prometheus alert rules
  ansible.builtin.copy:
    src: prometheus_alert_rules.yml
    dest: /etc/prometheus/alert_rules.yml
    owner: root
    group: root
    mode: '0644'
  notify: restart prometheus

- name: Enable and start Prometheus
  ansible.builtin.service:
    name: "{{ prometheus_svc }}"
    enabled: yes
    state: started

- name: Deploy Grafana ini (hardened)
  ansible.builtin.template:
    src: grafana.ini.j2
    dest: "{{ grafana_ini_path }}"
    owner: root
    group: root
    mode: '0644'
  notify: restart grafana

- name: Ensure Grafana dashboards directory exists
  ansible.builtin.file:
    path: /var/lib/grafana/dashboards
    state: directory
    owner: grafana
    group: grafana
    mode: '0755'
    
- name: Install example Grafana dashboard json
  ansible.builtin.copy:
    src: grafana_dashboard_config.json
    dest: /var/lib/grafana/dashboards/monitoring_dashboard.json
    owner: root
    group: root
    mode: '0644'

- name: Enable and start Grafana
  ansible.builtin.service:
    name: "{{ grafana_svc }}"
    enabled: yes
    state: started

- name: Deploy rsyslog configuration (forward to central server)
  ansible.builtin.template:
    src: rsyslog.conf.j2
    dest: "{{ rsyslog_conf_path }}"
    owner: root
    group: root
    mode: '0644'
  notify: restart rsyslog

- name: Enable and start rsyslog
  ansible.builtin.service:
    name: "{{ rsyslog_svc }}"
    enabled: yes
    state: started

- name: Install logrotate policy for Elasticsearch logs (example)
  ansible.builtin.copy:
    src: elasticsearch_logrotate.conf
    dest: /etc/logrotate.d/elasticsearch
    owner: root
    group: root
    mode: '0644'

- name: Install logrotate policy for Suricata logs
  ansible.builtin.copy:
    src: suricata_logrotate.conf
    dest: /etc/logrotate.d/suricata
    owner: root
    group: root
    mode: '0644'

- name: UFW - ensure enabled with default deny inbound
  community.general.ufw:
    state: enabled
    policy: "{{ ufw_default_input_policy }}"

- name: UFW - allow required TCP service ports
  community.general.ufw:
    rule: allow
    port: "{{ item }}"
    proto: tcp
    from: "{{ mgmt_allow_cidrs | join(',') }}"
  loop: "{{ ufw_allowed_tcp_ports }}"

- name: UFW - allow required UDP service ports
  community.general.ufw:
    rule: allow
    port: "{{ item }}"
    proto: udp
    from: "{{ mgmt_allow_cidrs | join(',') }}"
  loop: "{{ ufw_allowed_udp_ports }}"

- name: Optionally harden SSH daemon (PasswordAuthentication=no)
  ansible.builtin.lineinfile:
    path: /etc/ssh/sshd_config
    regexp: '^PasswordAuthentication'
    line: 'PasswordAuthentication no'
    create: no
  when: ssh_hardening_enable
  notify:
    - restart rsyslog  # placeholder; real deployment should restart ssh
